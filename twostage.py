# -*- coding: utf-8 -*-
"""Twostage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KkMETh_od8gmpYHUz3yLUc-XEMsrZS_g
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import average_precision_score
from transformers import CLIPProcessor, CLIPModel


clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")


video_dir = '/content/drive/mydrive/XD-Violence2'
label_map = {
    0: 'Fighting',
    1: 'Shooting',
    2: 'RIOT',
    3: 'Abuse',
    4: 'Car Accident',
    5: 'Explosion'
}

class VideoDataset(Dataset):
    def __init__(self, video_dir, label_map, num_frames=16):
        self.video_dir = video_dir
        self.label_map = label_map
        self.num_frames = num_frames
        self.data = self._prepare_data(video_dir)

    def _prepare_data(self, video_dir):
        data = []
        for label_id, label in self.label_map.items():
            folder_path = os.path.join(video_dir, str(label_id))
            for file_name in os.listdir(folder_path):
                if file_name.endswith('.mp4'):
                    data.append((os.path.join(folder_path, file_name), label_id))
        return data

    def _get_frames_from_video(self, video_path, num_frames):
        cap = cv2.VideoCapture(video_path)
        frames = []
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        step = max(total_frames // num_frames, 1)
        for i in range(0, total_frames, step):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame))
            if len(frames) >= num_frames:
                break
        cap.release()
        return frames

    def _generate_text_descriptions(self, frames, label):
        descriptions = [f"Frame {i + 1} of {label}." for i in range(len(frames))]
        return descriptions

    def __getitem__(self, idx):
        video_path, label_id = self.data[idx]
        frames = self._get_frames_from_video(video_path, self.num_frames)
        label = self.label_map[label_id]


        pixel_values = clip_processor(images=frames, return_tensors='pt', padding=True).pixel_values
        with torch.no_grad():
            image_features = clip_model.get_image_features(pixel_values)
        image_embedding = rwkv_model(image_features)


        text_descriptions = self._generate_text_descriptions(frames, label)
        texts_with_labels = [f"{description} {label}" for description in text_descriptions]
        random_embedding = torch.randn(len(texts_with_labels), 512)  # Example random embedding
        text_inputs = clip_processor(text=texts_with_labels, return_tensors='pt', padding=True).input_ids
        with torch.no_grad():
            text_features = clip_model.get_text_features(text_inputs) + random_embedding
        text_embedding = rwkv_model(text_features)

        return image_embedding, text_embedding, label_id

    def __len__(self):
        return len(self.data)


dataset = VideoDataset(video_dir, label_map)
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


class MultiClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(MultiClassifier, self).__init__()
        self.fc = nn.Linear(input_size, num_classes)

    def forward(self, x):
        return self.fc(x)


classifier = MultiClassifier(input_size=512, num_classes=len(label_map)).to('cuda' if torch.cuda.is_available() else 'cpu')
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)

def evaluate(data_loader, classifier, device='cuda'):
    classifier.eval()
    all_labels = []
    all_predictions = []
    with torch.no_grad():
        for image_embedding, text_embedding, label in data_loader:
            image_embedding = image_embedding.to(device)
            text_embedding = text_embedding.to(device)
            label = label.to(device)


            combined_embedding = (image_embedding + text_embedding) / 2


            logits = classifier(combined_embedding)


            all_predictions.extend(logits.cpu().numpy())
            all_labels.extend(label.cpu().numpy())


    ap = average_precision_score(all_labels, all_predictions, average='macro')
    return ap


def combined_loss(image_embedding, text_embedding, logits, labels, temperature=0.07, alpha=0.5):
    features = F.normalize(image_embedding, dim=1)
    contrastive_logits = torch.mm(features, features.T) / temperature
    labels_eq = torch.eq(labels, labels.T).float().to(features.device)
    mask = torch.ones_like(labels_eq) - torch.eye(labels.size(0), device=features.device)
    labels_eq = labels_eq * mask
    negatives = torch.logsumexp(contrastive_logits * (1 - labels_eq), dim=1, keepdim=True)
    contrastive_loss = -torch.mean((contrastive_logits - negatives) * labels_eq)
    cross_entropy_loss = F.cross_entropy(logits, labels)

    total_loss = alpha * contrastive_loss + (1 - alpha) * cross_entropy_loss
    return total_loss


def train_model(train_loader, classifier, optimizer, device='cuda', alpha=0.5):
    classifier.train()
    total_loss = 0
    for image_embedding, text_embedding, label in train_loader:
        image_embedding = image_embedding.to(device)
        text_embedding = text_embedding.to(device)
        label = label.to(device)
        optimizer.zero_grad()


        combined_embedding = (image_embedding + text_embedding) / 2


        logits = classifier(combined_embedding)
        loss = combined_loss(image_embedding, text_embedding, logits, label, alpha=alpha)

        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

num_epochs = 600
device = 'cuda' if torch.cuda.is_available() else 'cpu'
classifier.to(device)

for epoch in range(num_epochs):
    train_loss = train_model(train_loader, classifier, optimizer, device)
    train_ap = evaluate(train_loader, classifier, device)
    val_ap = evaluate(val_loader, classifier, device)
    test_ap = evaluate(test_loader, classifier, device)
    print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train AP: {train_ap:.4f}, Validation AP: {val_ap:.4f}, Test AP: {test_ap:.4f}")
# -*- coding: utf-8 -*-
"""BaseonMobileNet_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WLkobKIaS6p8Z8F67ExOSR5cJ31Jjrs7
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_dataset = datasets.ImageFolder(root='/content/Roblow/Violencedata/train', transform=transform)
test_dataset = datasets.ImageFolder(root='/content/Roblow/Violencedata/test', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)

class BaseonMobileNet(nn.Module):
    def __init__(self, num_classes):
        super(BaseonMobileNet, self).__init__()

        def conv_dw(in_channels, out_channels, stride):

            conv_depthwise = nn.Sequential(
                nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),
                nn.BatchNorm2d(in_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
            )

            deconv_depthwise = nn.Sequential(
                nn.ConvTranspose2d(out_channels, in_channels, 1, 1, 0, bias=False),
                nn.BatchNorm2d(in_channels),
                nn.ReLU(inplace=True),
                nn.ConvTranspose2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),
                nn.BatchNorm2d(in_channels),
                nn.ReLU(inplace=True),
            )
            return conv_depthwise, deconv_depthwise

        self.model = nn.Sequential(
            nn.Conv2d(3, 32, 3, 2, 1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        self.deconv_layers = nn.ModuleList([
            nn.ConvTranspose2d(32, 3, 3, 2, 1, bias=False),
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True)
        ])

        conv_blocks = [
            conv_dw(32, 64, 1),
            conv_dw(64, 128, 2),
            conv_dw(128, 128, 1),
            conv_dw(128, 256, 2),
            conv_dw(256, 256, 1),
            conv_dw(256, 512, 2),
            *[conv_dw(512, 512, 1) for _ in range(5)],
            conv_dw(512, 1024, 2),
            conv_dw(1024, 1024, 1),
        ]

        for conv_block in conv_blocks:
            conv_layer, deconv_layer = conv_block
            self.model.add_module('conv', conv_layer)
            self.deconv_layers.append(deconv_layer)

        self.model.add_module('adaptive_pool', nn.AdaptiveAvgPool2d(1))
        self.fc = nn.Linear(1024, num_classes)

    def forward(self, x):
        activations = []
        for conv_layer, deconv_layer in zip(self.model, self.deconv_layers):
            x = conv_layer(x)
            activations.append(x)

        x = x.view(-1, 1024)
        return self.fc(x), activations

model = MobileNetV1(num_classes=len(train_dataset.classes))
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train(model, loader, criterion, optimizer, device='cpu'):
    model.train()
    running_loss = 0.0
    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    return running_loss / len(loader)

def test(model, loader, criterion, device='cpu'):
    model.eval()
    correct = 0
    total = 0
    running_loss = 0.0
    all_labels = []
    all_predictions = []
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_predictions)
    recall = recall_score(all_labels, all_predictions, average='macro')
    f1 = f1_score(all_labels, all_predictions, average='macro')
    precision = precision_score(all_labels, all_predictions, average='macro')

    return running_loss / len(loader), accuracy, recall, f1, precision

num_epochs = 600

model_save_path = '/content/Newmobilenet_model.pth'

for epoch in range(num_epochs):
    train_loss = train(model, train_loader, criterion, optimizer, device)
    test_loss, accuracy, recall, f1, precision = test(model, test_loader, criterion, device)

    print(f'Epoch {epoch + 1}/{num_epochs}:')
    print(f'Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, '
          f'Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}')

torch.save(model.state_dict(), model_save_path)
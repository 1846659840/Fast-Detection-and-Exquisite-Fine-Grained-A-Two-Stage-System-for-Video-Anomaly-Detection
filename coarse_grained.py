# -*- coding: utf-8 -*-
"""Coarse_grained.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mza7NAQTnOquLSfBREL0AsAroXQfFI5q
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import os
import numpy as np
from sklearn.metrics import average_precision_score
from torch.utils.data import DataLoader, Dataset, random_split
from PIL import Image
from BaseonMobileNet_Training import BaseonMobileNet
import cv2
from torch.optim import Adam
from sklearn.metrics import average_precision_score
from torch.utils.data import DataLoader
from skopt import BayesSearchCV

mobilenet_model = BaseonMobileNet(num_classes=2)
mobilenet_model.load_state_dict(torch.load('/content/Newmobilenet_model.pth'))
mobilenet_model.eval()

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def extract_frame_features(frame):
    frame = transform(frame).unsqueeze(0)
    with torch.no_grad():
        activations = mobilenet_model(frame)
    return activations.view(-1).cpu().numpy()

def extract_video_features(video_frames):
    return [extract_frame_features(frame) for frame in video_frames]

def get_frames_from_video(video_path, frame_rate=3):
    cap = cv2.VideoCapture(video_path)
    frames = []
    count = 0
    while True:
        success, frame = cap.read()
        if not success:
            break
        if count % frame_rate == 0:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(frame))
        count += 1
    cap.release()
    return frames

def extract_features_from_videos(video_dir, label_map):
    video_features = []
    video_labels = []
    for label, sub_dir in label_map.items():
        sub_dir_path = os.path.join(video_dir, sub_dir)
        for video_file in os.listdir(sub_dir_path):
            if video_file.endswith(('.mp4', '.avi', '.mov', '.mkv')):
                video_path = os.path.join(sub_dir_path, video_file)
                frames = get_frames_from_video(video_path)
                features = extract_video_features(frames)
                video_features.append(features)
                video_labels.append(label)
    return video_features, video_labels

video_dir = '/content/drive/mydrive/XD-Violence'
label_map = {0: 'Normal', 1: 'Violence'}

video_features, video_labels = extract_features_from_videos(video_dir, label_map)

class VideoFeatureDataset(Dataset):
    def __init__(self, features, labels, group_size=3):
        self.features = features
        self.labels = labels
        self.group_size = group_size
        self.data = self.group_frames()

    def group_frames(self):
        grouped_data = []
        for features, label in zip(self.features, self.labels):
            num_groups = len(features) // self.group_size
            for i in range(num_groups):
                grouped_features = features[i * self.group_size:(i + 1) * self.group_size]
                grouped_data.append((grouped_features, label))
        return grouped_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        features, label = self.data[idx]
        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)

class SimpleCNN(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)
        self.fc = nn.Linear(256, output_size)
        self.relu = nn.ReLU()
        self.flatten = nn.Flatten()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x))
        x = self.flatten(x)
        x = self.fc(x)
        return x

dataset = VideoFeatureDataset(video_features, video_labels, group_size=3)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

class RWKV(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(RWKV, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.kw = nn.Parameter(torch.randn(hidden_size, input_size))
        self.vw = nn.Parameter(torch.randn(hidden_size, input_size))
        self.rw = nn.Parameter(torch.randn(hidden_size, input_size))
        self.ow = nn.Linear(hidden_size, output_size)

        self.time_mix_k = nn.Parameter(torch.rand(1))
        self.time_mix_v = nn.Parameter(torch.rand(1))
        self.time_mix_r = nn.Parameter(torch.rand(1))

        self.time_first = nn.Parameter(torch.randn(num_layers))
        self.time_decay = nn.Parameter(torch.randn(num_layers))


        self.register_buffer('state', torch.zeros(num_layers * 4, hidden_size))

    def forward(self, x):

        layer_state = self.state.clone()
        out = x

        for i in range(self.num_layers):
            xk = out * self.time_mix_k + layer_state[5 * i + 0] * (1 - self.time_mix_k)
            xv = out * self.time_mix_v + layer_state[5 * i + 0] * (1 - self.time_mix_v)
            xr = out * self.time_mix_r + layer_state[5 * i + 0] * (1 - self.time_mix_r)

            layer_state[5 * i + 0] = out


            r = torch.sigmoid(self.rw @ xr.T).T
            k = (self.kw @ xk.T).T
            v = (self.vw @ xv.T).T

            kk = k
            vv = v
            aa = layer_state[5 * i + 1]
            bb = layer_state[5 * i + 2]
            pp = layer_state[5 * i + 3]


            ww = self.time_first[i] + kk
            p = torch.maximum(pp, ww)
            e1 = torch.exp(pp - p)
            e2 = torch.exp(ww - p)
            a = e1 * aa + e2 * vv
            b = e1 * bb + e2
            wkv = a / b


            ww = pp + self.time_decay[i]
            p = torch.maximum(ww, kk)
            e1 = torch.exp(ww - p)
            e2 = torch.exp(kk - p)
            layer_state[5 * i + 1] = e1 * aa + e2 * vv
            layer_state[5 * i + 2] = e1 * bb + e2
            layer_state[5 * i + 3] = p


            out = r * wkv


        out = self.ow(out)
        return out

input_size = 1024
hidden_size = 512
num_layers = 2
output_size = 2
rwkv = RWKV(input_size * 3, hidden_size, num_layers, output_size).to('cuda' if torch.cuda.is_available() else 'cpu')

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(rwkv.parameters(), lr=0.001)

def train(model, dataloader, criterion, optimizer, device='cpu'):
    model.train()
    total_loss = 0
    for inputs, labels in dataloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs, 0)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate(model, dataloader, device='cpu'):
    model.eval()
    all_labels = []
    all_probs = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs.view(inputs.shape[0], -1))
            probs = nn.functional.softmax(outputs, dim=1)[:, 1].cpu().numpy()
            all_labels.extend(labels.numpy())
            all_probs.extend(probs)

    return average_precision_score(all_labels, all_probs)

def distillation_loss(student_outputs, teacher_outputs, labels, T):
    soft_teacher = nn.functional.softmax(teacher_outputs / T, dim=1)
    soft_student = nn.functional.log_softmax(student_outputs / T, dim=1)
    loss_kd = nn.functional.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T * T)
    loss_ce = nn.functional.cross_entropy(student_outputs, labels)
    return loss_kd + loss_ce

def train_with_kd(student_model, teacher_model, dataloader, criterion, optimizer, T, device='cpu'):
    student_model.train()
    teacher_model.eval()
    total_loss = 0
    for inputs, labels in dataloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        student_outputs = student_model(inputs)
        with torch.no_grad():
            teacher_outputs = teacher_model(inputs.view(inputs.shape[0], -1), 0)
        loss = distillation_loss(student_outputs, teacher_outputs, labels, T)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate_ap(model, dataloader, device='cpu'):
    model.eval()
    all_labels = []
    all_probs = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            probs = nn.functional.softmax(outputs, dim=1)[:, 1].cpu().numpy()
            all_labels.extend(labels.numpy())
            all_probs.extend(probs)
    return average_precision_score(all_labels, all_probs)

input_size = 1024
output_size = 2
student_model = SimpleCNN(input_size, output_size).to(device)
optimizer = Adam(student_model.parameters(), lr=0.001)

def train_optimize():
    search_space = {
        'T': (1, 20)
    }

    def objective(params):
        T = params['T']
        return train_with_kd(student_model, rwkv, train_loader, criterion, optimizer, T, device)

    optimizer_search = BayesSearchCV(estimator=objective, search_spaces=search_space, n_iter=10)
    optimizer_search.fit(train_loader)

    return optimizer_search.best_params_['T']


device = 'cuda' if torch.cuda.is_available() else 'cpu'
rwkv.to(device)

num_epochs = 600
for epoch in range(num_epochs):
    train_loss = train(rwkv, train_loader, criterion, optimizer, device)
    ap_score = evaluate(rwkv, val_loader, device)
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, AP Score: {ap_score:.4f}')
save_model(rwkv, '/content/drive/mydrive/rwkv_model_final.pth')

test_ap_score = evaluate(rwkv, test_loader, device)
print(f'Test AP Score: {test_ap_score:.4f}')

best_T = train_optimize()

for epoch in range(num_epochs):
    train_loss = train_with_kd(student_model, rwkv, train_loader, criterion, optimizer, best_T, device)
    val_ap = evaluate_ap(student_model, val_loader, device)
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val AP: {val_ap:.4f}')

torch.save(student_model.state_dict(), '/content/drive/mydrive/simple_cnn_student_model.pth')

test_ap = evaluate_ap(student_model, test_loader, device)
print(f'Test AP: {test_ap:.4f}')